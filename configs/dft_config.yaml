# DFT Training Configuration

num_epochs: 3
batch_size: 6
gradient_accumulation_steps: 4
learning_rate: 2.0e-5
warmup_ratio: 0.1
lr_scheduler_type: cosine
logging_steps: 10
save_steps: 500
save_total_limit: 3
max_seq_length: 512
packing: false
loss_type: dft
run_name: dft_baseline
